---

# Defaults vars file for airflow role

# Airflow installation tasks
airflow_system_dependencies: "{{ _airflow_system_dependencies }}"

# Set changed when due to idempotency problem with pip module
# Always changed with airflow and airflow[crypto]
airflow_pip_changed_when: false

# Ansible's pip module doesn't currently support complex version strings
# https://github.com/ansible/ansible/issues/19321
# And Airflow may currently require custom version strings
# https://stackoverflow.com/a/48075827
airflow_pip_custom_version_install: false

# Installation vars
airflow_user_name: 'airflow'
airflow_user_group: "{{ airflow_user_name }}"
airflow_user_shell: '/bin/bash'
airflow_user_home_path: '/var/lib/airflow'
airflow_application_home: "/var/lib/airflow/airflow"
airflow_user_home_mode: '0700'

airflow_log_path: '/var/log/airflow'
airflow_log_owner: "{{ airflow_user_name }}"
airflow_log_group: "{{ airflow_user_group }}"
airflow_log_mode: '0700'

airflow_pid_path: '/var/run/airflow'
airflow_pid_owner: "{{ airflow_user_name }}"
airflow_pid_group: "{{ airflow_user_group }}"
airflow_pid_mode: '0700'

airflow_virtualenv: "{{ airflow_user_home_path }}/venv"
airflow_virtualenv_mode: "0755"

airflow_python_version: "{{ _airflow_python_version }}"
# Update if python version or apache airflow version changes.
airflow_python_constraint: "--constraint=https://raw.githubusercontent.com/apache/airflow/a5cebdc94b7c1bb48bfd54898819db4905880b52/constraints-3.6.txt"

airflow_packages:
  - name: 'pip'
    version: '19.0.3'
  - name: 'setuptools'
    version: '39.1.0'
  - name: 'GitPython'
    version: '2.1.9'
  - name: 'Cython'
    version: '0.28.2'
  - name: 'apache-airflow'
    version: '1.10.14'
  - name: 'pipenv'
airflow_extra_packages:
  - name: 'apache-airflow[crypto]'

# option to set environment for all users,
# by providing a file in /etc/profile.d/
airflow_provide_user_env: true
airflow_profiled_template: "templates/airflow.profile.j2"

# SERVICES MANAGEMENT
# -----------------------------------------------------------------------------

# Airflow systemd services specific settings
is_systemd_managed_system: "{{ _is_systemd_managed_system | default(false) }}"
airflow_services_systemd:
  - dest: '/etc/systemd/system/airflow-webserver.service'
    handler: 'Restart airflow-webserver'
    options:
      Install:
        WantedBy: 'multi-user.target'
      Service:
        Environment: "PATH={{ airflow_virtualenv ~ '/bin' }}"
        EnvironmentFile: "{{ airflow_service_envfile_path }}"
        ExecStart: "{{ airflow_virtualenv ~ '/bin' }}/airflow webserver -l {{ airflow_log_path }}/webserver-l.log -A {{ airflow_log_path }}/webserver-access.log -E {{ airflow_log_path }}/webserver-errors.log"
        Group: "{{ airflow_user_group }}"
        PrivateTmp: 'true'
        Restart: 'on-failure'
        RestartSec: '5s'
        Type: 'simple'
        User: "{{ airflow_user_name }}"
      Unit:
        After: 'network.target postgresql.service mysql.service redis.service rabbitmq-server.service'
        Description: 'Airflow webserver daemon'
        Wants: 'postgresql.service mysql.service redis.service rabbitmq-server.service'
  - dest: '/etc/systemd/system/airflow-scheduler.service'
    handler: 'Restart airflow-scheduler'
    options:
      Install:
        WantedBy: 'multi-user.target'
      Service:
        Environment: "PATH={{ airflow_virtualenv ~ '/bin' }}"
        EnvironmentFile: "{{ airflow_service_envfile_path }}"
        ExecStart: "{{ airflow_virtualenv ~ '/bin' }}/airflow scheduler"
        Group: "{{ airflow_user_group }}"
        PrivateTmp: 'true'
        Restart: 'always'
        RestartSec: '5s'
        Type: 'simple'
        User: "{{ airflow_user_name }}"
      Unit:
        After: 'network.target postgresql.service mysql.service redis.service rabbitmq-server.service'
        Description: 'Airflow scheduler daemon'
        Wants: 'postgresql.service mysql.service redis.service rabbitmq-server.service'

airflow_services_states:
  - name: 'airflow-webserver'
    enabled: true
    state: 'started'
  - name: 'airflow-scheduler'
    enabled: true
    state: 'started'

# Environment variables file
airflow_service_envfile_template: 'templates/airflow.env.j2'
airflow_service_envfile_path: '/etc/default/airflow'

# Databases variables
airflow_manage_database: true
airflow_database_engine: 'mysql'
airflow_database_sqlite_file_path: "{{ airflow_user_home_path ~ '/airflow/airflow.db' }}"

# Set do_init to false if database already initialized
airflow_do_init_db: true
airflow_do_upgrade_db: true

# Default configuration
airflow_config_template: "templates/airflow.cfg.j2"
airflow_defaults_config:
  core:
    airflow_home_mode: "0700"
    dags_repos_folder: "{{ airflow_user_home_path ~ '/airflow/dags_repos' }}"
    dags_folder: "{{ airflow_user_home_path ~ '/airflow/dags' }}"
    dags_folder_mode: "0755"
    base_log_folder: "{{ airflow_log_path }}"
    remote_base_log_folder: ''
    remote_log_conn_id: ''
    encrypt_s3_logs: false
    executor: 'SequentialExecutor'
    sql_alchemy_conn: "sqlite:///{{ airflow_database_sqlite_file_path }}"
    sql_alchemy_pool_size: 5
    sql_alchemy_pool_recycle: 3600
    parallelism: 32
    dag_concurrency: 16
    dags_are_paused_at_creation: true
    non_pooled_task_slot_count: 128
    max_active_runs_per_dag: 16
    load_examples: false
    plugins_folder: "{{ airflow_user_home_path ~ '/airflow/plugins' }}"
    airflow_fernet_key: 'cryptography_not_found_storing_passwords_in_plain_text'
    donot_pickle: false
    dagbag_import_timeout: 30

  operators:
    default_owner: 'Airflow'

  webserver:
    auth_backend: ''
    authenticate: false
    base_url: 'http://localhost:8080'
    debug: false
    expose_config: true
    filter_by_owner: false
    hostname: "{{ ansible_default_ipv4.address }}"
    log_file: "{{ airflow_log_path }}/airflow-webserver.log"
    pid: "{{ airflow_pid_path }}/airflow-webserver.pid"
    rbac: false
    secret_key: 'temporary_key'
    web_server_host: '0.0.0.0'
    web_server_port: '8080'
    web_server_worker_timeout: 120
    workers: 1
    worker_class: 'sync'
    worker_timeout: 30

  email:
    email_backend: 'airflow.utils.email.send_email_smtp'

  smtp:
    smtp_host: 'localhost'
    smtp_starttls: true
    smtp_ssl: false
    smtp_user: 'airflow'
    smtp_port: 25
    smtp_password: 'airflow'
    smtp_mail_from: 'airflow@airflow.com'

  celery:
    celery_app_name: 'airflow.executors.celery_executor'
    worker_concurrency: 16
    worker_log_server_port: 8793
    broker_url: 'sqla+mysql://airflow:airflow@localhost:3306/airflow'
    celery_result_backend: 'db+mysql://airflow:airflow@localhost:3306/airflow'
    flower_port: 5555
    default_queue: 'default'

  scheduler:
    job_heartbeat_sec: 5
    log_file: "{{ airflow_log_path }}/airflow-scheduler.log"
    pid: "{{ airflow_pid_path }}/airflow-scheduler.pid"
    scheduler_heartbeat_sec: 5
    statsd_on: false
    statsd_host: 'localhost'
    statsd_port: 8125
    statsd_prefix: 'airflow'
    parsing_processes: 2

  mesos:
    master: 'localhost:5050'
    framework_name: 'Airflow'
    task_cpu: 1
    task_memory: 256
    checkpoint: false
    failover_timeout: 604800
    authenticate: false
    default_principal: 'admin'
    default_secret: 'admin'

  ldap:
    uri: 'ldaps://<your.ldap.server>:<port>'
    user_filter: 'objectClass=*'
    user_name_attr: 'uid'
    superuser_filter: ''
    data_profiler_filter: ''
    bind_user: ''
    bind_password: ''
    basedn: 'dc=example,dc=com'
    cacert: ''
    search_scope: 'LEVEL'

  google:
    client_id: "your generated client id"
    client_secret: "your generated client secret"
    oauth_callback_route: "/oauth2callback"
    domain: "your.airflow.domain.edu"

airflow_user_config: {}
airflow_config: "{{ airflow_defaults_config | combine(airflow_user_config, recursive=true) }}"

# Airflow Webserver Users management
airflow_webserver_admins:
  - google_username: "google_113716629440137414774"
    email: "tuj27080@temple.edu"
    firstname: "David"
    lastname: "Kinzer"
    airflow_dummy_password: "dummy-password"

# Connections management
airflow_connections:
  - conn_id: "ANSIBLE_TEST"
    conn_uri: "http://localhost:9999"

# Variables management
airflow_variables:
  - key: "ANSIBLE_TEST"
    value: "test_value"

# Logrotate configuration
airflow_logrotate_template: "templates/logrotate.j2"
airflow_logrotate_config:
  - filename: '/etc/logrotate.d/airflow'
    log_pattern:
      - "{{ airflow_log_path }}/*.log"
    options:
      - 'rotate 12'
      - 'weekly'
      - 'compress'
      - 'delaycompress'
      - "create 640 {{ airflow_log_owner }} {{ airflow_log_group }}"
      - 'postrotate'
      - 'endscript'

# Git setup for cloning DAGs into Airflow DAG folder
workspace: "{{ airflow_config.core.dags_folder }}"
dags_git_repositories:
  - repo: "https://github.com/tulibraries/cob_datapipeline.git"
    dest_folder: "cob_datapipeline"
    branch_or_release: "main"
dags_related_git_repositories:
  - repo: "https://github.com/tulibraries/tul_cob.git"
    dest_folder: "$HOME/tul_cob"
    branch_or_release: "main"

airflow_dag_system_dependencies:
  - name: "@Development tools"
    state: "present"
  - name: "sqlite-devel"
    state: "present"
